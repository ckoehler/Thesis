\citet{Daley} and \citet{GhilMal} refer 
to this problem as the ``data assimilation'' problem.  But I can 
write this sentence a different way:  Consequently, this problem 
is referred to as the ``data assimilation'' problem 
\citep{Daley,GhilMal}.  I will let the text run on here a bit so that
a big reference list will be generated.

Formally, let $c$ be a control vector of size $m$ and $S$ denote the 
feasible region for the control vector.  For any $c$ in $S$, let 
$J(c)$ denote the weighted sum of the squared difference between the 
observation and the solution of the model corresponding to the control 
vector $c$.  Except in trivial cases the explicit form of $J$ as a 
function of $c$ is not known.  It is to be emphasized that there are 
other possible choices for the $J$ function.  One may be interested 
only in the state of the model at a given time instant, say $t = 
\Delta$.  Whatever be the nature and type of the $J$ function, 
mathematically, the data assimilation problem can be stated as follows 
: find a $c^{*}$ in $S$ such that $J(c^{*})$ is a minimum, that is, we 
are lead to an optimization problem under the dynamical constraints of 
the model equations.  Since $J$ is a ``smooth'' function, one method 
for finding $c^{*}$ is to use one of the many variants of the 
classical gradient method.  This is however, more easily said than 
done.  The difficulty primarily stems from the fact that $J$ is not 
known explicitly.  A now popular method for finding the gradient of 
$J$ is called the ``adjoint'' method \citep{GhilMal,ThackLong}.  A 
summary of data assimilation using the adjoint method is shown in Fig.  
\ref{flowchart} and described below:


Considerable success has been reported in the literature in the use of 
adjoint method for finding $c^{*}$  
\citep{Luenberger,SunFlick,Wolfsberg}.  The success of this combination is largely 
dependent on the properties of the $J$ function.  This approach can 
succeed only if $J$ is unimodal in $S$.  It turns out that the 
modality of $J$ critically depends on the model dynamics.  It is now 
known that the nonlinearity in the model dynamics induces 
multimodality in the $J$ function \citep{Li,Chung}.

There are at least two factors affecting the rate of convergence of 
the iterates leading to the optimum value of the $J$ function.  First,
is the number and distribution of the observations.  There is a minimum 
number of observations required from an algorithmic viewpoint, but 
satisfaction of this requirement is insufficient to guarantee a solution.
The distribution of these data (in space and time), in concert wtih 
the dynamics, dictates the existence of a solution.  Second, is the shape 
of the $J$ function.  Judicious choice of scaling can remove 
eccentricities in the $J$-field.  It is thus imperative 
to understand the role of these two factors affecting the iterates.

The effect of the number and distribution of observations on the 
quality of the 
iterates are often examined using controlled experiments which have come 
to be known as the ``twin'' experiments.  In this, a point in the 
feasible region for the control vector is first chosen and then the 
model solution is calculated for this value of the 
control vector .  Then observations (including known error) are generated from 
the model solution by adding noise with known characteristics.  
By computing the optimal estimate of the control vector for different 
sets of observations, we can develop a better understanding 
of the dependence of the optimal estimate on the number, distribution
 and accuracy of observations.

As for the shape of the $J$ function, since it is not known 
explicitly, we must be contented with the analysis of the 
properties of $J$ around the local minima.  This is often done 
by approximating $J$ around the optimum $c^{*}$ using a quadratic form 
such as \be J(c) = \frac{1}{2}c^{t} H c + p^{t} c + q , \ee where $H$ 
is the Hessian matrix which is a symmetric matrix of the second 
derivatives of $J$ with respect to $c$, \be p = \left( p_{1} , p_{2} , 
p_{3} , p_{4} , ...  , p_{n}\right)^{t} \ee is a vector and $q$ is a 
constant.  By analyzing the eigenvalues of $H$, we can draw inferences 
on the shape of $J$ in the vicinity of $c^{*}$ \citep{Thacker}.  For 
example, if one of the eigenvalues of $H$ is very small, then the 
contours or the level curves of $J$ for the valley around $c^{*}$ are 
elongated ellipses.  This would imply that the iterative process of 
locating the minimum would converge slowly and with difficulty.

In this dissertation, one aim is to apply the adjoint method to the 
mixed layer model \citep{Ball,Lilly}.  This model is used to predict 
the return flow of the warm, humid air from the Gulf of Mexico into 
the coastal plains during the winter months.  The mixed layer model 
has a small number of unknown variables/parameters.  Athough small, the model
is nonlinear and thus presents difficulties in dynamical optimization.
With the experience gained, we wanted to analyze the computational aspects of 
data assimilation.  This brought us to the shallow water equations 
\citep{Pedlosky}, which has a larger number of variables in the
discrete model.  We are interested in solving some of the 
computational aspects of this model that are relevant to data 
assimilation.  Since the solution of the forward model and the adjoint 
take a good fraction of the efforts, we turned our attention to 
parallel methods for solving this class of equations.

The shallow water model is discretized first using the Euler scheme.
  But due to its instability, we also examined
 the solution using a stable 
leapfrog scheme.  Both types of discretization resulted 
in a block bi-diagonal system of equations for the forward model and 
the adjoint model.  We are interested in examining the comparative 
analysis of four vector/parallel algorithms in solving this system of 
equations.
These four vector/parallel algorithms belong to a class of direct 
parallel methods.  The first algorithm is also known as the ``divide and 
conquer'' method \citep{Varahan}.  The second algorithm is a variation
of the divide and conquer method \citep
{Varahan,Conn,MeyerPod87,Vandervorst,MeyerPod89}.  The third 
algorithm is
known as the partition algorithm \citep{Vandervorst89}. The fourth 
algorithm is the
cyclic reduction method \citep{Varahan}. A comparison of these four 
vector/parallel algorithms is done on the CrayJ90 in scalar mode and 
using 1,2,4 and 8 vector processors.

 This dissertation is organized as follows.  A description of the use
 of encapsulated postscript files is presented in Chapter 
 \ref{eps_pdf_examp}.  One way to present bargraphs is shown in
 model equations are described in Section \ref{bargraph}.  Some example
 tables are shown in Section \ref{table}.  




